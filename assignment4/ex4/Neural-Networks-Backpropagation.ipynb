{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task we need to accomplish consists in recognizing handwritten digits (0 to 9).\n",
    "\n",
    "In this notebook we will implement the whole Neural Network algorithm, hence **Feed Forward Propagation** (defining the relation between predictors and output) plus **Back Propagation**.\n",
    "\n",
    "In the previous notebook we implemented just the *Feed Forward Propagation* step using pre-trained weights.<br>\n",
    "In this notebook instead we will learn how to learn these weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start with some theory notions first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's represent again how the *Forward Propagation* step is computed in the Neural Network:\n",
    "<img src=\"img/NN.png\" alt=\"Neural Network representation\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's define some variables first:\n",
    "1. $L$ = number of layers\n",
    "2. $s_l$ = number of neurons (nodes) in the $l_{th}$ layer not counting the bias unit\n",
    "3. $K$ = number of classes, $S_L = K$\n",
    "4. $m$ = number of data points (row) in the training set\n",
    "5. $n$ = number of features (columns in the training sets)\n",
    "\n",
    "Second, let's recall the cost function for Logistic Regression:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m(-y^{(i)} log(h^{(i)}(\\theta)) - (1-y^{(i)})log(1-h^{(i)}(\\theta))) + \\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost for the NN is going to be just a more complicated version of the above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K\\big[-y_k^{(i)} log((h_{\\Theta}(x^{(i)}))_k) - (1-y_k^{(i)})log(1-(h_{\\Theta}(x^{(i)}))_k)\\big] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l + 1}(\\Theta_{i,j}^{(l)})^2$\n",
    "\n",
    "where $h_{\\theta}(x^{(i)})$ is computed as shown in the Figure 2 above, and $K$ = 10 is the total number of possible labels. <br>\n",
    "Note that $h_{\\theta}(x^{(i)})_k = a_k^{(3)}$ is the activation function (output value) of the *k*-th output unit (in the output layer) for the *i*-th training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find $\\Theta$ to minimize $J(\\Theta)$.\n",
    "\n",
    "To do that, we need to calculate $\\nabla J(\\Theta)$ w.r.t. (with respect to) to $\\Theta$, basically $\\frac{\\partial J(\\Theta)}{\\partial \\Theta_{i,j}^{(l)}} = D_{i,j}^{(l)}$. <br>\n",
    "The **backpropagation algorithm** helps us to calculate $D_{i,j}^{(l)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: we are going to explain how the backpropagation algorithm works taking into account that our Neural Network has **3 layers** (one input layer, one hidden layer and one output layer) where the input layer has **400** units (excluding bias term), the hidden layer has **25** units (excluding bias term) and the output layer has **10** units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\Delta^{(l)} := 0$, define a gradient matrix $\\Delta$ for each layer, excluding the input layer (so do not define $\\Delta^{(0)}$) and set its values to zero. Concretely, in our case we will define $\\Delta^{(2)}$ and $\\Delta^{(1)}$. The size of $\\Delta^{(l)}$ is euqual to:\n",
    "$$ size(\\Delta^{(l)}) = size(\\theta^{(l)})$$\n",
    "These $\\Delta^{(l)}$ matrix will be used to store the gradient of the cost function.\n",
    "\n",
    "\n",
    "2. For $i = 1:m$ (looping through the training set): <br>\n",
    "   A- set the input layer's values ($a^{(1)}$) to the $t$-th trainig example $x^{(t)}$. <br>\n",
    "   \n",
    "   B- perform a **feed forward propagation** pass (Figure 2), computing the activations ($z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)}$) for layers 2 and 3. Note that you need to add $+1$ term to ensure that the vectors of activations for layers $a^{(1)}$ and $a^{(2)}$ also include the bias unit. <br>\n",
    "   \n",
    "   C- Then, for each node $j$ in layer $l$, we would like to compute an \"error term\" $\\delta_j^{(l)}$ that measures *how much that node was \"responsible\" for any errors in our output\"*. For each output unit $k$ in layer 3 (the output layer), set:\n",
    "   $$\\delta_k^{(3)} = (a_k^{(3)} - y_k)$$ where $y_k \\in \\{0, 1\\}$ indicates whether the current training example belongs to class $k$ ($y_k = 1$), or if it belongs to a different class ($y_k = 0$). <br>\n",
    "   \n",
    "   D- for hidden layer $l = 2$, set:\n",
    "   $$\\delta^{(2)} = (\\theta^{(2)})^T \\delta^{(3)}.* g'(z^{(2)}) = (\\theta^{(2)})^T \\delta^{(3)}.* ((a^{(2)}).*(1-a^{(2)}))$$\n",
    "   For the **input unit** we do not compute the error term. (Note that $\".*\"$ indicates normal multiplication and not matrix multiplication). <br>\n",
    "   \n",
    "   E- Accumulate the gradient using the following formula: \n",
    "   $$\\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l + 1)}(a^{(l)})^T $$\n",
    "   So, in our case, we have calculted so far $\\delta^{(3)}$, $\\delta^{(2)}$ and we have $\\Delta^{(2)}$ and $\\Delta^{(1)}$ which we have previously initialized. Then the updates of $\\Delta^{(2)}$ and $\\Delta^{(1)}$ are:\n",
    "   $$\\Delta^{(2)} = \\Delta^{(2)} + \\delta^{(3)}(a^{(2)})^T $$\n",
    "   $$\\Delta^{(1)} = \\Delta^{(1)} + \\delta^{(2)}(a^{(1)})^T $$\n",
    "   Note that you should skip or remove $\\delta_0^{(2)}$ (the bias term).\n",
    "\n",
    "\n",
    "3. Obtain the regularized gradient for the Neural Network cost function by dividing the accumulated gradients by $\\frac{1}{m}$:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\Theta_{i,j}^{(l)}} J(\\Theta)= D_{i,j}^{(l)} = \\frac{1}{m}\\Delta_{i, j}^{(l)}$ for $j = 0$ <br>\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\Theta_{i,j}^{(l)}} J(\\Theta)= D_{i,j}^{(l)} = \\frac{1}{m}\\Delta_{i, j}^{(l)} + \\frac{\\lambda}{m} \\theta_{i, j}^{(l)}$ for $j \\geq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following picture is depicted how the *error terms* are computed in backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/backpropNN.png\" alt=\"Backpropagation Neural Network representation\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know how to implement the Neural Network cost function and its gradient computation. Then, in order to learn the $\\theta$ parameters, we can use some optimization algorithm to find a set of $\\theta$ parameters which minimize our cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z)*(1 -sigmoid(z))\n",
    "\n",
    "def backPropagationNNCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamd):\n",
    "    Theta1 = nn_params[0:((input_layer_size + 1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size + 1)\n",
    "    Theta2 = nn_params[((input_layer_size + 1) * hidden_layer_size):].reshape(num_labels, hidden_layer_size + 1)\n",
    "    #Theta1 = nn_params[0:hidden_layer_size * (input_layer_size + 1)].reshape(hidden_layer_size, input_layer_size + 1)\n",
    "    #Theta2 = nn_params[hidden_layer_size * (input_layer_size + 1):].reshape(num_labels, hidden_layer_size + 1) \n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J = 0\n",
    "    \n",
    "    # need to recode y\n",
    "    y_recoded = np.zeros((y.shape[0], num_labels))\n",
    "    for i in range(y.shape[0]):\n",
    "        y_recoded[i, y[i, 0]-1] = 1\n",
    "    \n",
    "    # add bias column to X\n",
    "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    # feed forward propagation for each training example\n",
    "    for i in range(m):\n",
    "        a1 = X[i, :].reshape(-1, 1).T #1, 401\n",
    "        a2 = sigmoid((np.dot(Theta1, a1.T))).reshape(-1, 1).T #(1, 25)\n",
    "        a2 = np.c_[np.ones((a2.shape[0], 1)), a2] #(1, 26)\n",
    "        h = sigmoid((np.dot(Theta2, a2.T))).reshape(-1, 1).T #(1, 10)\n",
    "        # compute cost function\n",
    "        J = J + (1/m)*((np.dot(-y_recoded[i, :], np.log(h.T))) - (np.dot(1 - y_recoded[i, :], np.log(1 - h.T)))) \n",
    "    \n",
    "    #add regularization\n",
    "    J = J + (lamd/(2*m))*((sum(sum(Theta1[:, 1:]**2))) + (sum(sum(Theta2[:, 1:]**2))))\n",
    "    return np.asscalar(J.squeeze())\n",
    "    \n",
    "def randInitializeWeights(L_in, L_out):\n",
    "    epsilon_init = 0.12\n",
    "    return np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "def backPropagationGradient(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamd):\n",
    "    Theta1 = nn_params[0:((input_layer_size + 1) * hidden_layer_size)].reshape(hidden_layer_size, input_layer_size + 1)\n",
    "    Theta2 = nn_params[((input_layer_size + 1) * hidden_layer_size):].reshape(num_labels, hidden_layer_size + 1)\n",
    "    #Theta1 = nn_params[0:hidden_layer_size * (input_layer_size + 1)].reshape(hidden_layer_size, input_layer_size + 1)\n",
    "    #Theta2 = nn_params[hidden_layer_size * (input_layer_size + 1):].reshape(num_labels, hidden_layer_size + 1)\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # need to recode y\n",
    "    y_recoded = np.zeros((y.shape[0], num_labels))\n",
    "    for i in range(y.shape[0]):\n",
    "        y_recoded[i, y[i, 0]-1] = 1\n",
    "    \n",
    "    # add bias column to X\n",
    "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    # initialize gradient matrix\n",
    "    Theta1_grad = np.zeros((Theta1.shape[0], Theta1.shape[1])) # (25, 401)\n",
    "    Theta2_grad = np.zeros((Theta2.shape[0], Theta2.shape[1])) # (10, 26)\n",
    "    \n",
    "    for i in range(m):\n",
    "        # feed forward propagation\n",
    "        a1 = X[i, :].reshape(-1, 1).T #1, 401\n",
    "        a2 = sigmoid((np.dot(Theta1, a1.T))).reshape(-1, 1).T #(1, 25)\n",
    "        a2 = np.c_[np.ones((a2.shape[0], 1)), a2] #(1, 26)\n",
    "        h = sigmoid((np.dot(Theta2, a2.T))).reshape(-1, 1).T #(1, 10)\n",
    "        \n",
    "        delta_3 = h - y_recoded[i, :] #(1, 10)\n",
    "        delta_2 = np.dot(delta_3, Theta2)*sigmoidGradient(a2) #(1, 26) .* (1, 26)\n",
    "        \n",
    "        Theta2_grad = Theta2_grad + np.dot(a2.T, delta_3).T\n",
    "        Theta1_grad = Theta1_grad + np.dot(delta_2.T[1:], a1) #(26, 1)*(1, 401)\n",
    "    \n",
    "    # put bias column to zero because we do not have to regularize it\n",
    "    Theta1[:, 0] = 0\n",
    "    Theta2[:, 0] = 0\n",
    "    \n",
    "    Theta1_grad = (1/m)*Theta1_grad + (lamd/m)*Theta1\n",
    "    Theta2_grad = (1/m)*Theta2_grad + (lamd/m)*Theta2\n",
    "    \n",
    "    grad = np.vstack((Theta1_grad.reshape(Theta1_grad.size, 1), Theta2_grad.reshape(Theta2_grad.size, 1)))\n",
    "    return grad.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400 # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25  # 25 hidden units\n",
    "num_labels = 10         # 10 labels, from 1 to 10 (note that we have mapped \"0\" to label 10)\n",
    "\n",
    "data = sio.loadmat('ex4data1.mat')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "data = sio.loadmat('ex4weights.mat')\n",
    "Theta1 = data['Theta1']\n",
    "Theta2 = data['Theta2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38376986])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Theta1[:, :].reshape(-1, 1)\n",
    "b = Theta2[:, :].reshape(-1, 1)\n",
    "# rolling of parameters\n",
    "nn_params = np.append(a, b).reshape(-1, 1)\n",
    "lamd = 1\n",
    "cost = backPropagationNNCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamd)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = backPropagation(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_grad(backPropagationNNCostFunction, backPropagationGradient , nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lamd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400 # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25  # 25 hidden units\n",
    "num_labels = 10\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "nn_params = np.append(initial_Theta1, initial_Theta2).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10285, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer_size  = 400 # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25  # 25 hidden units\n",
    "num_labels = 10\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "nn_params = np.append(initial_Theta1, initial_Theta2).reshape(-1, 1)\n",
    "nn_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000095\n",
      "         Iterations: 7\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 24\n"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "InputHiddenInit = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "HiddenOutputInit = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "initial_nn_weights = np.vstack((InputHiddenInit.reshape(InputHiddenInit.size, 1), \n",
    "                                HiddenOutputInit.reshape(HiddenOutputInit.size, 1)))\n",
    "\n",
    "opt_results = optimize.fmin_cg(f=backPropagationNNCostFunction, x0=initial_nn_weights.squeeze(), \n",
    "                                     args=(input_layer_size, hidden_layer_size, num_labels, X[0:50, :], y[0:50, :], lamb),\n",
    "                                     fprime=backPropagationGradient, disp=True, full_output=True, retall=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    m = X.shape[0]\n",
    "    num_labels = Theta2.shape[0]\n",
    "    # value to return\n",
    "    predictions = np.zeros((m, 1)) #50, 1\n",
    "    # add bias therm for first layer\n",
    "    a1 = np.c_[np.ones((m, 1)), X] #50, 401\n",
    "    \n",
    "    # calculate a2 using sigmoid function\n",
    "    a2 = sigmoid(Theta1.dot(a1.T)).T # 50, 25\n",
    "    # add bias therm for second layer\n",
    "    a2 = np.c_[np.ones((a2.shape[0], 1)), a2] #50, 26\n",
    "    \n",
    "    # calculate output (h == a3)\n",
    "    h = sigmoid(a2.dot(Theta2.T)) # 50, 10 \n",
    "    \n",
    "    # calculate prediction choosing the index of max argument for each row\n",
    "    predictions = np.argmax(h, axis=1) + 1\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "optimized_nn_weights = opt_results[0]\n",
    "\n",
    "mat4 = sio.loadmat('ex4data1.mat')\n",
    "X = mat4['X']\n",
    "y = mat4['y']\n",
    "X = X[0:50, :]\n",
    "y = y[0:50, :]\n",
    "input_layer_size  = 400 # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25  # 25 hidden units\n",
    "num_labels = 10\n",
    "Theta1 = optimized_nn_weights[0:hidden_layer_size * (input_layer_size + 1)].reshape(hidden_layer_size, input_layer_size + 1)\n",
    "Theta2 = optimized_nn_weights[hidden_layer_size * (input_layer_size + 1):].reshape(num_labels, hidden_layer_size + 1)\n",
    "\n",
    "\n",
    "# feed forward propagation\n",
    "Theta1.shape, Theta2.shape\n",
    "p = predict(Theta1, Theta2, X)\n",
    "accuracy = (y == p.reshape(-1,1)).mean()*100\n",
    "\n",
    "print('NN Accuracy: {0}%'.format(round(accuracy, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
